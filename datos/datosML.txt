50000 iteraciones
10000/10000 [==============================] - 213s 21ms/step - reward: 0.1028
174 episodes - episode_reward: 5.922 [-43.010, 46.628] - loss: 0.010 - mae: 0.246 - mean_q: 0.405 - mean_eps: 0.896

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 218s 22ms/step - reward: 0.4314
224 episodes - episode_reward: 19.251 [-40.090, 46.919] - loss: 0.011 - mae: 0.262 - mean_q: 0.428 - mean_eps: 0.715

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 218s 22ms/step - reward: 0.7211
260 episodes - episode_reward: 27.741 [-42.740, 47.346] - loss: 0.012 - mae: 0.269 - mean_q: 0.444 - mean_eps: 0.525

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 225s 22ms/step - reward: 1.0736
308 episodes - episode_reward: 34.838 [-40.160, 47.185] - loss: 0.013 - mae: 0.274 - mean_q: 0.452 - mean_eps: 0.335

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 229s 23ms/step - reward: 1.2476
done, took 1104.469 seconds
Results against random player:
DQN Evaluation: 99 victories out of 100 episodes
Results against max base power player:
DQN Evaluation: 78 victories out of 100 episodes
Evaaaaaaa  <coroutine object evaluate_player at 0x000001DDE025BAC0>
Evaluation with included method: <coroutine object evaluate_player at 0x000001DDE025BAC0>
Cross evaluation of DQN with baselines:
------------------  -------------------  ------------------  -------------------  -------------------
-                   SimpleRLPlayer 3     RandomPlayer 4      MaxBasePowerPlay 2   SimpleHeuristics 1
SimpleRLPlayer 3                         0.9666666666666667  0.7333333333333333   0.16666666666666666
RandomPlayer 4      0.03333333333333333                      0.06666666666666667  0.03333333333333333
MaxBasePowerPlay 2  0.26666666666666666  0.9333333333333333                       0.06666666666666667
SimpleHeuristics 1  0.8333333333333334   0.9666666666666667  0.9333333333333333
------------------  -------------------  ------------------  -------------------  -------------------


100000 iteraciones
10000/10000 [==============================] - 221s 22ms/step - reward: 0.0403
170 episodes - episode_reward: 2.389 [-43.940, 44.196] - loss: 0.010 - mae: 0.246 - mean_q: 0.404 - mean_eps: 0.948

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 235s 24ms/step - reward: 0.3111
197 episodes - episode_reward: 15.779 [-42.310, 45.146] - loss: 0.011 - mae: 0.262 - mean_q: 0.431 - mean_eps: 0.858

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 238s 24ms/step - reward: 0.2885
207 episodes - episode_reward: 13.936 [-42.900, 46.299] - loss: 0.011 - mae: 0.274 - mean_q: 0.454 - mean_eps: 0.763

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 244s 24ms/step - reward: 0.5395
232 episodes - episode_reward: 23.227 [-40.480, 47.459] - loss: 0.012 - mae: 0.283 - mean_q: 0.471 - mean_eps: 0.668

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 246s 25ms/step - reward: 0.7591
262 episodes - episode_reward: 28.947 [-37.730, 47.391] - loss: 0.012 - mae: 0.287 - mean_q: 0.480 - mean_eps: 0.573



Interval 8 (70000 steps performed)
10000/10000 [==============================] - 254s 25ms/step - reward: 1.2176
330 episodes - episode_reward: 36.883 [-37.360, 47.904] - loss: 0.014 - mae: 0.300 - mean_q: 0.500 - mean_eps: 0.288

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 255s 25ms/step - reward: 1.2743
348 episodes - episode_reward: 36.629 [-35.540, 47.782] - loss: 0.014 - mae: 0.298 - mean_q: 0.494 - mean_eps: 0.193

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 249s 25ms/step - reward: 1.2767
done, took 2452.125 seconds
Results against random player:
2023-07-09 17:25:32,346 - SimpleRLPlayer 3 - WARNING - Received pm: |pm| SimpleRLPlayer 3|~|/error /choose - must be used in a chat room, not a console





10000/10000 [==============================] - 234s 23ms/step - reward: -0.0352
169 episodes - episode_reward: -2.074 [-42.910, 45.466] - loss: 0.010 - mae: 0.275 - mean_q: 0.476 - mean_eps: 0.965

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 245s 24ms/step - reward: 0.1742
184 episodes - episode_reward: 9.486 [-42.390, 45.327] - loss: 0.011 - mae: 0.250 - mean_q: 0.417 - mean_eps: 0.905

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 261s 26ms/step - reward: 0.1997
186 episodes - episode_reward: 10.710 [-42.950, 46.976] - loss: 0.011 - mae: 0.262 - mean_q: 0.433 - mean_eps: 0.842

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 264s 26ms/step - reward: 0.4002
214 episodes - episode_reward: 18.702 [-42.520, 47.158] - loss: 0.011 - mae: 0.265 - mean_q: 0.443 - mean_eps: 0.778

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 276s 28ms/step - reward: 0.4815
214 episodes - episode_reward: 22.499 [-42.000, 46.353] - loss: 0.011 - mae: 0.276 - mean_q: 0.461 - mean_eps: 0.715

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 272s 27ms/step - reward: 0.6754
243 episodes - episode_reward: 27.795 [-45.010, 46.609] - loss: 0.012 - mae: 0.279 - mean_q: 0.473 - mean_eps: 0.652

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 257s 26ms/step - reward: 0.7297
252 episodes - episode_reward: 28.948 [-40.510, 47.284] - loss: 0.012 - mae: 0.286 - mean_q: 0.484 - mean_eps: 0.588

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 260s 26ms/step - reward: 0.9011
283 episodes - episode_reward: 31.810 [-39.850, 47.930] - loss: 0.013 - mae: 0.291 - mean_q: 0.492 - mean_eps: 0.525

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 261s 26ms/step - reward: 0.8538
282 episodes - episode_reward: 30.292 [-39.460, 47.607] - loss: 0.012 - mae: 0.290 - mean_q: 0.491 - mean_eps: 0.462

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 264s 26ms/step - reward: 1.0381
299 episodes - episode_reward: 34.718 [-39.500, 47.728] - loss: 0.013 - mae: 0.292 - mean_q: 0.493 - mean_eps: 0.398

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 266s 27ms/step - reward: 1.1059
306 episodes - episode_reward: 36.149 [-38.320, 47.038] - loss: 0.013 - mae: 0.292 - mean_q: 0.490 - mean_eps: 0.335

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 268s 27ms/step - reward: 1.2400
336 episodes - episode_reward: 36.898 [-37.410, 47.608] - loss: 0.013 - mae: 0.295 - mean_q: 0.496 - mean_eps: 0.272

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 269s 27ms/step - reward: 1.3086
345 episodes - episode_reward: 37.938 [-33.000, 47.668] - loss: 0.014 - mae: 0.296 - mean_q: 0.495 - mean_eps: 0.208

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 271s 27ms/step - reward: 1.3688
350 episodes - episode_reward: 39.115 [-40.070, 47.835] - loss: 0.014 - mae: 0.300 - mean_q: 0.502 - mean_eps: 0.145

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 273s 27ms/step - reward: 1.3582
done, took 3942.141 seconds
Training done and saved.


unning them or create a new session.
10000/10000 [==============================] - 221s 22ms/step - reward: 0.0282
162 episodes - episode_reward: 1.739 [-44.000, 45.218] - loss: 0.010 - mae: 0.240 - mean_q: 0.398 - mean_eps: 0.974

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 225s 22ms/step - reward: 0.0424
185 episodes - episode_reward: 2.289 [-43.140, 46.368] - loss: 0.010 - mae: 0.260 - mean_q: 0.429 - mean_eps: 0.929

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 239s 24ms/step - reward: 0.1479
189 episodes - episode_reward: 7.829 [-42.880, 43.607] - loss: 0.011 - mae: 0.266 - mean_q: 0.439 - mean_eps: 0.881

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 250s 25ms/step - reward: 0.1931
185 episodes - episode_reward: 10.425 [-41.060, 46.370] - loss: 0.011 - mae: 0.263 - mean_q: 0.434 - mean_eps: 0.834

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 254s 25ms/step - reward: 0.3893
210 episodes - episode_reward: 18.546 [-42.390, 46.618] - loss: 0.011 - mae: 0.272 - mean_q: 0.454 - mean_eps: 0.786

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 262s 26ms/step - reward: 0.4167
220 episodes - episode_reward: 18.924 [-42.300, 46.831] - loss: 0.011 - mae: 0.277 - mean_q: 0.461 - mean_eps: 0.739

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 266s 27ms/step - reward: 0.5898
232 episodes - episode_reward: 25.399 [-42.310, 47.062] - loss: 0.012 - mae: 0.280 - mean_q: 0.471 - mean_eps: 0.691

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 268s 27ms/step - reward: 0.5930
231 episodes - episode_reward: 25.707 [-40.800, 47.668] - loss: 0.012 - mae: 0.284 - mean_q: 0.479 - mean_eps: 0.644

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 264s 26ms/step - reward: 0.6600
247 episodes - episode_reward: 26.763 [-43.320, 47.452] - loss: 0.012 - mae: 0.285 - mean_q: 0.481 - mean_eps: 0.596

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 264s 26ms/step - reward: 0.6723
257 episodes - episode_reward: 26.130 [-42.360, 47.426] - loss: 0.012 - mae: 0.287 - mean_q: 0.484 - mean_eps: 0.549

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 270s 27ms/step - reward: 0.8345
268 episodes - episode_reward: 31.105 [-39.840, 47.525] - loss: 0.012 - mae: 0.285 - mean_q: 0.478 - mean_eps: 0.501

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 281s 28ms/step - reward: 0.9454
293 episodes - episode_reward: 32.254 [-39.690, 47.115] - loss: 0.013 - mae: 0.293 - mean_q: 0.493 - mean_eps: 0.454

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 283s 28ms/step - reward: 1.0078
300 episodes - episode_reward: 33.636 [-35.400, 47.120] - loss: 0.013 - mae: 0.294 - mean_q: 0.495 - mean_eps: 0.406

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 289s 29ms/step - reward: 1.1175
316 episodes - episode_reward: 35.327 [-38.330, 47.748] - loss: 0.013 - mae: 0.294 - mean_q: 0.493 - mean_eps: 0.359

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 302s 30ms/step - reward: 1.1915
324 episodes - episode_reward: 36.760 [-38.140, 47.747] - loss: 0.013 - mae: 0.299 - mean_q: 0.505 - mean_eps: 0.311

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 3332 episodes - episode_reward: 36.545 [-40.540, 47.340] - loss: 0.014 - mae: 0.301 - mean_q: 0.508 - mean_eps: 0.264

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 308s 31ms/step - reward: 1.2752
344 episodes - episode_reward: 37.079 [-37.010, 47.910] - loss: 0.014 - mae: 0.305 - mean_q: 0.512 - mean_eps: 0.216

Interval 18 (170000 steps performed)
 8909/10000 [=========================>....] - E 8911/10000 [=========================>....] - E 8912/10000 [=========================>....] - E 8914/10000 [=========================>....] - E 8916/10000 [=========================>....] - E 8918/10000 [=========================>....] - E 8920/10000 [=========================>....] - E 8922/10000 [=========================>....] - E 8924/10000 [=========================>....] - E 8926/10000 [=========================>....] - E 8928/10000 [=========================>....] - E 8930/10000 [=========================>....] - E 8932/10000 [=========================>....] - E 8934/10000 [=========================>....] - E 8936/10000 [=========================>....] - E 8938/10000 [=========================>....] - E 8940/10000 [=========================>....] - E 8942/10000 [=========================>....] - E 8944/10000 [=========================>....] - E 8946/10000 [=========================>....] - E 8948/10000 [=========================>....] - E 8950/10000 [=========================>....] - E 8952/10000 [=========================>....] - E 8954/10000 [=========================>....] - E 8956/10000 [=========================>....] - E 8958/10000 [=========================>....] - E 8960/10000 [=========================>....] - E 8962/10000 [=========================>....] - E 8963/10000 [=========================>....] - E 8964/10000 [=========================>....] - E 8966/10000 [=========================>....] - E 8968/10000 [=========================>....] - E 8970/10000 [=========================>....] - E 8972/10000 [=========================>....] - E 8974/10000 [=========================>....] - E 8976/10000 [=========================>....] - E 8978/10000 [=========================>....] - E 8980/10000 [=========================>....] - E 8982/10000 [=========================>....] - E 8984/10000 [=========================>....] - E 8986/10000 [=========================>....] - E 8988/10000 [=========================>....] - E 8990/10000 [=========================>....] - E 8992/10000 [=========================>....] - E 8994/10000 [=========================>....] - E 8995/10000 [=========================>....] - E 8997/10000 [=========================>....] - E 8999/10000 [=========================>....] - E 9001/10000 [==========================>...] - E 9003/10000 [==========================>...] - E 9005/10000 [==========================>...] - E 9007/10000 [==========================>...] - E 9009/10000 [==========================>...] - E 9011/10000 [==========================>...] - E 9013/10000 [==========================>...] - E 9015/10000 [==========================>...] - E 9017/10000 [==========================>...] - E 9019/10000 [==========================>...] - E 9021/10000 [==========================>...] - E 9023/10000 [==========================>...] - E 9025/10000 [==========================>...] - E 9027/10000 [==========================>...] - E 9028/10000 [==========================>...] - E 9030/10000 [==========================>...] - E 9032/10000 [==========================>...] - ETA: 9034/10000 [==========================>...] - ETA: 29s - 9036/10000 [==========================>...] - ETA: 29s - 9038/10000 [==========================>...] - ETA: 29s - 9040/10000 [==========================>...] - ETA: 29s - 9042/10000 [==========================>...] - ETA: 29s - 9044/10000 [==========================>...] - ETA: 29s - 9046/10000 [==========================>...] - ETA: 29s - 9048/10000 [==========================>...] - ETA: 29s - 9050/10000 [==========================>...] - ETA: 29s - 9051/10000 [==========================>...] - ETA: 29s - 9053/10000 [==========================>...] - ETA: 29s - 9055/10000 [==========================>...] - ETA: 29s - 9057/10000 [==========================>...] - ETA: 29s - 9059/10000 [==========================>...] - ETA: 29s - 9061/10000 [==========================>...] - ETA: 28s - 9063/10000 [==========================>...] - ETA: 28s - 9065/10000 [==========================>...] - ETA: 28s 10000/10000 [==============================] - 315s 32ms/step - reward: 1.2872
343 episodes - episode_reward: 37.525 [-35.520, 47.711] - loss: 0.014 - mae: 0.302 - mean_q: 0.506 - mean_eps: 0.169

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 333s 33ms/step - reward: 1.3241
353 episodes - episode_reward: 37.529 [-38.820, 47.967] - loss: 0.015 - mae: 0.306 - mean_q: 0.513 - mean_eps: 0.121

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 308s 31ms/step - reward: 1.3881
done, took 5506.294 seconds
Training done and saved.
Results against random player:



10000/10000 [==============================] - 239s 24ms/step - reward: 0.0045
172 episodes - episode_reward: 0.222 [-46.350, 47.046] - loss: 0.010 - mae: 0.254 - mean_q: 0.420 - mean_eps: 0.979

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 236s 24ms/step - reward: 0.0291
174 episodes - episode_reward: 1.695 [-46.350, 46.205] - loss: 0.011 - mae: 0.246 - mean_q: 0.395 - mean_eps: 0.943

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 234s 23ms/step - reward: 0.0384
183 episodes - episode_reward: 2.112 [-43.540, 46.392] - loss: 0.011 - mae: 0.246 - mean_q: 0.398 - mean_eps: 0.905

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 251s 25ms/step - reward: 0.1219
193 episodes - episode_reward: 6.300 [-43.740, 46.111] - loss: 0.011 - mae: 0.237 - mean_q: 0.382 - mean_eps: 0.867

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 261s 26ms/step - reward: 0.2678
201 episodes - episode_reward: 13.324 [-45.900, 46.204] - loss: 0.011 - mae: 0.254 - mean_q: 0.416 - mean_eps: 0.829

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 265s 27ms/step - reward: 0.2733
211 episodes - episode_reward: 12.971 [-42.530, 46.946] - loss: 0.011 - mae: 0.265 - mean_q: 0.437 - mean_eps: 0.791

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 261s 26ms/step - reward: 0.3990
213 episodes - episode_reward: 18.727 [-40.410, 46.749] - loss: 0.011 - mae: 0.273 - mean_q: 0.454 - mean_eps: 0.753

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 266s 27ms/step - reward: 0.4538
226 episodes - episode_reward: 20.096 [-43.560, 46.732] - loss: 0.012 - mae: 0.280 - mean_q: 0.468 - mean_eps: 0.715

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 262s 26ms/step - reward: 0.5081
232 episodes - episode_reward: 21.897 [-40.810, 47.107] - loss: 0.012 - mae: 0.284 - mean_q: 0.4

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 279s 28ms/step - reward: 0.5642
234 episodes - episode_reward: 24.051 [-42.880, 46.572] - loss: 0.012 - mae: 0.284 - mean_q: 0.477 - mean_eps: 0.639

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 281s 28ms/step - reward: 0.7001
251 episodes - episode_reward: 27.959 [-42.860, 47.660] - loss: 0.012 - mae: 0.285 - mean_q: 0.481 - mean_eps: 0.601

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 301s 30ms/step - reward: 0.7747
262 episodes - episode_reward: 29.535 [-40.310, 48.000] - loss: 0.012 - mae: 0.287 - mean_q: 0.484 - mean_eps: 0.563

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 292s 29ms/step - reward: 0.7970
275 episodes - episode_reward: 28.982 [-38.210, 47.225] - loss: 0.012 - mae: 0.290 - mean_q: 0.493 - mean_eps: 0.525

Interval 14 (130000 steps performed)
 4378/10000 [============>.................] - E 4380/10000 [============>.................] - E 4382/10000 [============>.................] - E 4384/10000 [============>.................] - E 4386/10000 [============>.................] - E 4388/10000 [============>.................] - E 4390/10000 [============>.................] - E 4392/10000 [============>.................] - E 4394/10000 [============>.................] - E 4396/10000 [============>.................] - E 4398/10000 [============>.................] - E 4400/10000 [============>.................] - E 4402/10000 [============>.................] - E 4404/10000 [============>.................] - E 4406/10000 [============>.................] - E 4407/10000 [============>.................] - E 4409/10000 [============>.................] - E 4411/10000 [============>.................] - E 4413/10000 [============>.................] - E 4415/10000 [============>.................] - E 4417/10000 [============>.................] - E 4419/10000 [============>.................] - E 4421/10000 [============>.................] - E 4423/10000 [============>.................] - E 4425/10000 [============>.................] - E 4427/10000 [============>.................] - E 4429/10000 [============>.................] - E 4431/10000 [============>.................] - E 4433/10000 [============>.................] - E 4435/10000 [============>.................] - E 4437/10000 [============>.................] - E 4439/10000 [============>................ 4440/10000 [============>.................] - ETA: 2:44  4441/10000 [============>.................] - ETA: 2:44  4442/10000 [============>.................] - ETA: 2:44 : 2:44  4447/10000 [============>.................] - ETA: 2:44  4449/10000 [============>.................] - ETA: 2:44  4451/10000 [============>.................] - ETA: 2:44  4453/10000 [============>.................] - ETA: 2:44  4455/10000 [============>.................] - ETA: 2:44  4457/10000 [============>.................] - ETA: 2:44  4459/10000 [============>.................] - ETA: 2:44  4461/10000 [============>.................] - ETA: 2:44  4463/10000 [============>.................] - ETA: 2:44  4465/10000 [============>.................] - ETA: 2:44  4467/10000 [============>.................] - ETA: 2:44  4468/10000 [============>.................] - ETA: 2:44  4470/10000 [============>.................] - ETA: 2:43  4472/10000 [============>.................] - ETA: 2:43  4474/10000 [============>.................] - ETA: 2:43  4476/10000 [============>.................] - ETA: 2:43  4478/10000 [============>.................] - ETA: 2:43  4480/10000 [============>.................] - ETA: 2:43  4482/10000 [============>.................] - ETA: 2:43  4484/10000 [============>.................] - ETA: 2:43  4486/10000 [============>.................] - ETA: 2:43  4488/10000 [...] - ETA: 2:09 - r10000/10000 [==============================] - 304s 30ms/step - reward: 0.8585
274 episodes - episode_reward: 31.345 [-38.100, 47.377] - loss: 0.013 - mae: 0.291 - mean_q: 0.491 - mean_eps: 0.487

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 294s 29ms/step - reward: 0.9908
10000/10000 [==============================] - 298s 30ms/step - reward: 1.0227
295 episodes - episode_reward: 34.664 [-37.420, 47.733] - loss: 0.013 - mae: 0.296 - mean_q: 0.501 - mean_eps: 0.411

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 299s 30ms/step - reward: 1.0009
299 episodes - episode_reward: 33.476 [-40.260, 47.238] - loss: 0.013 - mae: 0.297 - mean_q: 0.501 - mean_eps: 0.373

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 303s 30ms/step - reward: 1.0435
305 episodes - episode_reward: 34.194 [-39.830, 48.000] - loss: 0.013 - mae: 0.294 - mean_q: 0.496 - mean_eps: 0.335

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 299s 30ms/step - reward: 1.1754
320 episodes - episode_reward: 36.749 [-37.860, 47.927] - loss: 0.014 - mae: 0.293 - mean_q: 0.492 - mean_eps: 0.297

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 290s 29ms/step - reward: 1.1814
323 episodes - episode_reward: 36.577 [-40.740, 47.607] - loss: 0.014 - mae: 0.295 - mean_q: 0.498 - mean_eps: 0.259

Interval 21 (200000 steps performed)
10000/10000 [==============================] - 292s 29ms/step - reward: 1.2148
335 episodes - episode_reward: 36.262 [-35.260, 47.822] - loss: 0.014 - mae: 0.296 - mean_q: 0.498 - mean_eps: 0.221

Interval 22 (210000 steps performed)
10000/10000 [==============================] - 294s 29ms/step - reward: 1.3314
351 episodes - episode_reward: 37.907 [-40.630, 47.679] - loss: 0.014 - mae: 0.296 - mean_q: 0.497 - mean_eps: 0.183

Interval 23 (220000 steps performed)
10000/10000 [==============================] - 295s 30ms/step - reward: 1.3793
361 episodes - episode_reward: 38.241 [-42.360, 48.000] - loss: 0.014 - mae: 0.298 - mean_q: 0.499 - mean_eps: 0.145

Interval 24 (230000 steps performed)
10000/10000 [==============================] - 298s 30ms/step - reward: 1.4209
364 episodes - episode_reward: 39.030 [-43.060, 47.971] - loss: 0.014 - mae: 0.296 - mean_q: 0.495 - mean_eps: 0.107

Interval 25 (240000 steps performed)
10000/10000 [==============================] - 298s 30ms/step - reward: 1.3336
done, took 6993.820 seconds
Training done and saved.
Results against random player: