########################
### 100k iteraciones ###
########################
10000/10000 [==============================] - 173s 17ms/step - reward: -1.5620
380 episodes - episode_reward: -41.106 [-48.000, 36.396]
Interval 2
10000/10000 [==============================] - 206s 21ms/step - reward: -1.4901
367 episodes - episode_reward: -40.592 [-48.000, 32.756] - loss: 0.007 - mae: 0.393 - mean_q: -0.196 - mean_eps: 0.857

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 206s 21ms/step - reward: -1.5737
384 episodes - episode_reward: -40.976 [-48.000, 33.000] - loss: 0.007 - mae: 0.407 - mean_q: -0.217 - mean_eps: 0.763

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 208s 21ms/step - reward: -1.5395
379 episodes - episode_reward: -40.605 [-48.000, 39.864] - loss: 0.008 - mae: 0.373 - mean_q: -0.201 - mean_eps: 0.668

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 209s 21ms/step - reward: -1.5096
386 episodes - episode_reward: -39.122 [-48.000, 36.611] - loss: 0.008 - mae: 0.381 - mean_q: -0.211 - mean_eps: 0.573

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 206s 21ms/step - reward: -1.5470
384 episodes - episode_reward: -40.306 [-48.000, 38.875] - loss: 0.008 - mae: 0.403 - mean_q: -0.235 - mean_eps: 0.477

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 217s 22ms/step - reward: -1.5041
388 episodes - episode_reward: -38.741 [-48.000, 36.678] - loss: 0.008 - mae: 0.404 - mean_q: -0.231 - mean_eps: 0.383

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 213s 21ms/step - reward: -1.4642
389 episodes - episode_reward: -37.664 [-48.000, 41.182] - loss: 0.008 - mae: 0.395 - mean_q: -0.219 - mean_eps: 0.288

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 215s 22ms/step - reward: -1.3893
382 episodes - episode_reward: -36.360 [-48.000, 38.012] - loss: 0.008 - mae: 0.388 - mean_q: -0.200 - mean_eps: 0.192

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 212s 21ms/step - reward: -1.5039
done, took 2065.405 seconds

########################
### 200k iteraciones ###
########################

 mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
10000/10000 [==============================] - 189s 19ms/step - reward: -1.5508
374 episodes - episode_reward: -41.450 [-48.000, 35.026]

Interval 2 (10000 steps performed)
    1/10000 [..............................] - ETA: 3:19 - reward: 0.0000e+002023-07-10 23:55:25.108902: W tensorflow/c/c_api.cc:300] Operation '{name:'loss_1/AddN' id:250 op device:{requested: '', assigned: ''} def:{{{node loss_1/AddN}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul, loss_1/mul_1)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
2023-07-10 23:55:25.139333: W tensorflow/c/c_api.cc:300] Operation '{name:'training/Adam/Initial/kernel/m/Assign' id:455 op device:{requested: '', assigned: ''} def:{{{node training/Adam/Initial/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/Initial/kernel/m, training/Adam/Initial/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.
10000/10000 [==============================] - 238s 24ms/step - reward: -1.5787
380 episodes - episode_reward: -41.560 [-48.000, 32.336] - loss: 0.008 - mae: 0.495 - mean_q: -0.023 - mean_eps: 0.929

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 229s 23ms/step - reward: -1.5770
382 episodes - episode_reward: -41.267 [-48.000, 37.943] - loss: 0.008 - mae: 0.458 - mean_q: -0.137 - mean_eps: 0.881
_eps: 0.834

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 254s 25ms/step - reward: -1.5730
379 episodes - episode_reward: -41.491 [-48.000, -32.060] - loss: 0.008 - mae: 0.412 - mean_q: -0.212 - mean_eps: 0.786

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 256s 26ms/step - reward: -1.5657
380 episodes - episode_reward: -41.229 [-48.000, 38.367] - loss: 0.007 - mae: 0.397 - mean_q: -0.224 - mean_eps: 0.739

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 244s 24ms/step - reward: -1.5206
375 episodes - episode_reward: -40.528 [-48.000, 40.553] - loss: 0.007 - mae: 0.391 - mean_q: -0.239 - mean_eps: 0.691

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 252s 25ms/step - reward: -1.5532
382 episodes - episode_reward: -40.680 [-48.000, 37.493] - loss: 0.008 - mae: 0.418 - mean_q: -0.261 - mean_eps: 0.644

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 250s 25ms/step - reward: -1.5522
380 episodes - episode_reward: -40.815 [-48.000, 35.099] - loss: 0.008 - mae: 0.402 - mean_q: -0.247 - mean_eps: 0.596

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 238s 24ms/step - reward: -1.4986
374 episodes - episode_reward: -40.087 [-48.000, 41.042] - loss: 0.008 - mae: 0.384 - mean_q: -0.227 - mean_eps: 0.549

Interval 11 (100000 steps performed)
10000/10000 [==============================] - 237s 24ms/step - reward: -1.4698
376 episodes - episode_reward: -39.079 [-48.000, 35.144] - loss: 0.008 - mae: 0.385 - mean_q: -0.218 - mean_eps: 0.501

Interval 12 (110000 steps performed)
10000/10000 [==============================] - 240s 24ms/step - reward: -1.5059
383 episodes - episode_reward: -39.336 [-47.990, 35.744] - loss: 0.008 - mae: 0.413 - mean_q: -0.246 - mean_eps: 0.454

Interval 13 (120000 steps performed)
10000/10000 [==============================] - 240s 24ms/step - reward: -1.5169
391 episodes - episode_reward: -38.788 [-48.000, 35.647] - loss: 0.008 - mae: 0.425 - mean_q: -0.265 - mean_eps: 0.406

Interval 14 (130000 steps performed)
10000/10000 [==============================] - 246s 25ms/step - reward: -1.4486
371 episodes - episode_reward: -39.041 [-48.000, 35.886] - loss: 0.008 - mae: 0.369 - mean_q: -0.218 - mean_eps: 0.359

Interval 15 (140000 steps performed)
10000/10000 [==============================] - 248s 25ms/step - reward: -1.4377
367 episodes - episode_reward: -39.198 [-48.000, 35.722] - loss: 0.008 - mae: 0.378 - mean_q: -0.227 - mean_eps: 0.311

Interval 16 (150000 steps performed)
10000/10000 [==============================] - 264s 26ms/step - reward: -1.5253
382 episodes - episode_reward: -39.920 [-48.000, 45.266] - loss: 0.008 - mae: 0.407 - mean_q: -0.257 - mean_eps: 0.264

Interval 17 (160000 steps performed)
10000/10000 [==============================] - 268s 27ms/step - reward: -1.4731
379 episodes - episode_reward: -38.872 [-48.000, 40.980] - loss: 0.008 - mae: 0.419 - mean_q: -0.250 - mean_eps: 0.216

Interval 18 (170000 steps performed)
10000/10000 [==============================] - 256s 26ms/step - reward: -1.5399
378 episodes - episode_reward: -40.739 [-48.000, 37.903] - loss: 0.007 - mae: 0.465 - mean_q: -0.291 - mean_eps: 0.169

Interval 19 (180000 steps performed)
10000/10000 [==============================] - 255s 26ms/step - reward: -1.5114
381 episodes - episode_reward: -39.654 [-48.000, 35.295] - loss: 0.008 - mae: 0.426 - mean_q: -0.257 - mean_eps: 0.121

Interval 20 (190000 steps performed)
10000/10000 [==============================] - 260s 26ms/step - reward: -1.5275
done, took 4904.121 seconds
Training done and saved.
Results against random player: