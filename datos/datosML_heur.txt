#### 50k iteraciones #####
10000/10000 [==============================] - 222s 22ms/step - reward: -1.5480
375 episodes - episode_reward: -41.279 [-48.000, 34.310] - loss: 0.037 - mae: 0.240 - mean_q: 0.056 - mean_eps: 0.896

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 225s 23ms/step - reward: -1.5107
382 episodes - episode_reward: -39.539 [-48.000, 34.701] - loss: 0.019 - mae: 0.254 - mean_q: -0.053 - mean_eps: 0.715

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 227s 23ms/step - reward: -1.5010
382 episodes - episode_reward: -39.295 [-48.000, 35.590] - loss: 0.019 - mae: 0.267 - mean_q: -0.062 - mean_eps: 0.525

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 220s 22ms/step - reward: -1.4878
396 episodes - episode_reward: -37.566 [-48.000, 40.628] - loss: 0.020 - mae: 0.282 - mean_q: -0.074 - mean_eps: 0.335

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 224s 22ms/step - reward: -1.3285
Cross evaluation of DQN with baselines:
------------------  ----------------  --------------  ------------------  ------------------
-                   SimpleRLPlayer 3  RandomPlayer 2  MaxBasePowerPlay 2  SimpleHeuristics 3
SimpleRLPlayer 3                      0.82            0.5                 0.05
RandomPlayer 2      0.18                              0.07                0.02
MaxBasePowerPlay 2  0.5               0.93                                0.02
SimpleHeuristics 3  0.95              0.98            0.98
------------------  ----------------  --------------  ------------------  ------------------
##############################
##### 100k iteraciones #######
##############################
10000/10000 [==============================] - 220s 22ms/step - reward: -1.5902
382 episodes - episode_reward: -41.608 [-48.000, 35.219] - loss: 0.019 - mae: 0.285 - mean_q: -0.004 - mean_eps: 0.948

Interval 2 (10000 steps performed)
10000/10000 [==============================] - 223s 22ms/step - reward: -1.5826
382 episodes - episode_reward: -41.429 [-48.000, 35.123] - loss: 0.019 - mae: 0.294 - mean_q: -0.037 - mean_eps: 0.858

Interval 3 (20000 steps performed)
10000/10000 [==============================] - 223s 22ms/step - reward: -1.5371
375 episodes - episode_reward: -41.016 [-48.000, 33.000] - loss: 0.020 - mae: 0.300 - mean_q: -0.059 - mean_eps: 0.763

Interval 4 (30000 steps performed)
10000/10000 [==============================] - 224s 22ms/step - reward: -1.5696
391 episodes - episode_reward: -40.144 [-48.000, 39.784] - loss: 0.019 - mae: 0.307 - mean_q: -0.081 - mean_eps: 0.668

Interval 5 (40000 steps performed)
10000/10000 [==============================] - 226s 23ms/step - reward: -1.5098
392 episodes - episode_reward: -38.501 [-48.000, 43.089] - loss: 0.019 - mae: 0.307 - mean_q: -0.083 - mean_eps: 0.573

Interval 6 (50000 steps performed)
10000/10000 [==============================] - 228s 23ms/step - reward: -1.4720
384 episodes - episode_reward: -38.337 [-48.000, 36.882] - loss: 0.020 - mae: 0.309 - mean_q: -0.079 - mean_eps: 0.478

Interval 7 (60000 steps performed)
10000/10000 [==============================] - 227s 23ms/step - reward: -1.5239
392 episodes - episode_reward: -38.885 [-48.000, 44.138] - loss: 0.019 - mae: 0.315 - mean_q: -0.094 - mean_eps: 0.383

Interval 8 (70000 steps performed)
10000/10000 [==============================] - 238s 24ms/step - reward: -1.4517
398 episodes - episode_reward: -36.458 [-48.000, 43.192] - loss: 0.020 - mae: 0.314 - mean_q: -0.087 - mean_eps: 0.288

Interval 9 (80000 steps performed)
10000/10000 [==============================] - 243s 24ms/step - reward: -1.4267
389 episodes - episode_reward: -36.664 [-47.920, 41.192] - loss: 0.020 - mae: 0.320 - mean_q: -0.098 - mean_eps: 0.193

Interval 10 (90000 steps performed)
10000/10000 [==============================] - 240s 24ms/step - reward: -1.3075
Cross evaluation of DQN with baselines:
------------------  ----------------  --------------  ------------------  ------------------
-                   SimpleRLPlayer 3  RandomPlayer 2  MaxBasePowerPlay 2  SimpleHeuristics 3
SimpleRLPlayer 3                      0.93            0.51                0.1
RandomPlayer 2      0.07                              0.15                0.0
MaxBasePowerPlay 2  0.49              0.85                                0.07
SimpleHeuristics 3  0.9               1.0             0.93
------------------  ----------------  --------------  ------------------  ------------------
#############################
##### 200k iteraciones ######
#############################